{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an AI model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import neccessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data set into numpy array\n",
    "In this session, we are using \"Housing Dataset of 5000 people staying in USA\" provided by kaggle in the form of an .csv file\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/darshanprabhu09/housing-dataset-of-5000-people-staying-in-usa?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the csv file\n",
    "#You only need to run this code block ONCE when you first downloaded the source dataset.\n",
    "#Because (somehow) the original csv file is unusable, we need to do some formatting with simple read/write operations.\n",
    "srcfile = open('USA_Housing.csv','r+')\n",
    "destfile = open('USA_Housing_Fixed.csv','w')\n",
    "\n",
    "srclist = srcfile.readlines()\n",
    "for i in range(len(srclist)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i % 2 != 0:\n",
    "        destfile.write(srclist[i].rpartition(\",\\\"\")[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('USA_Housing_Fixed.csv',delimiter=',',dtype='float64')\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the prices column, which is the value we want to predict (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dataset[:, 5]\n",
    "dataset = np.delete(dataset, 5, 1)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test, targets_train, targets_test = train_test_split(dataset, targets, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model's basis for evaluation\n",
    "- Using cosine similarity as our evaluation method.\n",
    "- We calculate the cosine similarity of the input vector with multiple weight vectors, then we add all results together. (Layer 1)\n",
    "- Because the given dataset follows Gaussian probability (continous values), we need to pass the result to an activation function. (Layer 2)\n",
    "- For this session, our activation function is Sigmoid activation function\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- If the output is greater than 0.5, we return TRUE. Otherwise FALSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "def derivative_tanh(x):\n",
    "     return 1 - np.square(tanh(x))\n",
    "def make_prediction(input_vector, weights, bias):\n",
    "    layer_1 = np.dot(input_vector, weights)/(np.linalg.norm(input_vector)*np.linalg.norm(weights)) + bias\n",
    "    layer_2 = sigmoid(layer_1)\n",
    "    return layer_2\n",
    "def _cosine_similarity(input_vector, weights):\n",
    "        return np.dot(input_vector, weights)/(np.linalg.norm(input_vector)*np.linalg.norm(weights))\n",
    "input_vector = dataset[5]\n",
    "weights = np.array([2.5,3,1,-0.6,2])\n",
    "bias = np.array([0.0])\n",
    "result = _cosine_similarity(input_vector, weights) + bias\n",
    "#result = input_vector/(np.linalg.norm(input_vector) * np.linalg.norm(weights)) - _cosine_similarity(input_vector, weights)*(weights/np.square(np.linalg.norm(weights)))\n",
    "tanh(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the neural network model class\n",
    "- Because the data values are quite large, we cannot compute using the dot product.\n",
    "- Instead, the **cosine_similarity** is used.\n",
    "- Thank you to https://math.stackexchange.com/questions/1923613/partial-derivative-of-cosine-similarity for providing the formula to the derivative of cosine similarity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, learning_rate) -> None:\n",
    "        self.weights = np.array([np.random.randn(), np.random.randn(), np.random.rand(), np.random.randn(), np.random.randn()])\n",
    "        self.bias = np.random.randn()\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def _derivative_sigmoid(self, x):\n",
    "        return self._sigmoid(x) * (1-self._sigmoid(x))\n",
    "    \n",
    "    def _cosine_similarity(self, input_vector):\n",
    "        return np.dot(input_vector, self.weights)/(np.linalg.norm(input_vector)*np.linalg.norm(self.weights))\n",
    "    def _derivative_cosine_similarity_byWeights(self, input_vector):\n",
    "        return input_vector/(np.linalg.norm(input_vector) * np.linalg.norm(self.weights)) - self._cosine_similarity(input_vector)*(self.weights/np.square(np.linalg.norm(self.weights)))\n",
    "    \n",
    "    def _tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    def _derivative_tanh(self, x):\n",
    "        return 1 - np.square(tanh(x))\n",
    "    \n",
    "    def predict(self, input_vector):\n",
    "        layer_1 = self._cosine_similarity(input_vector)\n",
    "        layer_2 = self._sigmoid(layer_1 * 5) * self.bias\n",
    "        return layer_2 \n",
    "    \n",
    "    def _compute_gradients(self, input_vector, target):\n",
    "        layer_1 = self._cosine_similarity(input_vector)\n",
    "        layer_2 = self._sigmoid(layer_1 * 5) * self.bias\n",
    "        prediction = layer_2\n",
    "\n",
    "        derror_dprediction = 2 * (prediction - target) #MSE\n",
    "        dprediction_dlayer1 = self._derivative_sigmoid(layer_1 * 5) * self.bias * 5\n",
    "        dprediction_dbias = self._sigmoid(layer_1 * 5) \n",
    "        dlayer1_dweights = self._derivative_cosine_similarity_byWeights(input_vector)\n",
    "\n",
    "        derror_dbias = (\n",
    "            derror_dprediction * dprediction_dbias\n",
    "        )\n",
    "        derror_dweights = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    "        )\n",
    "        return derror_dbias, derror_dweights\n",
    "\n",
    "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
    "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
    "        self.weights = self.weights - (\n",
    "            derror_dweights * self.learning_rate\n",
    "        )\n",
    "        return derror_dbias, derror_dweights\n",
    "    \n",
    "    def train(self, input_vectors, targets, epochs=5):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        input_vectors: array of input vectors\n",
    "        targets: array of target values that corresponds to input vectors\n",
    "        epochs: number of iterations (default is 5)\n",
    "        \"\"\"\n",
    "        cumulative_errors = []\n",
    "        for current_iteration in range(epochs):\n",
    "            # Pick a data instance at random\n",
    "            random_data_index = np.random.randint(len(input_vectors))\n",
    "\n",
    "            input_vector = input_vectors[random_data_index]\n",
    "            target = targets[random_data_index]\n",
    "\n",
    "            # Compute the gradients and update the weights\n",
    "            derror_dbias, derror_dweights = self._compute_gradients(\n",
    "                input_vector, target\n",
    "            )\n",
    "\n",
    "            self._update_parameters(derror_dbias, derror_dweights)\n",
    "\n",
    "            # Measure the cumulative error for all the instances\n",
    "            if current_iteration % 100 == 0:\n",
    "                cumulative_error = 0\n",
    "                # Loop through all the instances to measure the error\n",
    "                for data_instance_index in range(len(input_vectors)):\n",
    "                    data_point = input_vectors[data_instance_index]\n",
    "                    target = targets[data_instance_index]\n",
    "\n",
    "                    prediction = self.predict(data_point)\n",
    "                    error = np.square(prediction - target)\n",
    "\n",
    "                    cumulative_error = cumulative_error + error\n",
    "                cumulative_errors.append(cumulative_error)\n",
    "        return cumulative_errors\n",
    "    \n",
    "    def fit(self, input_vectors, targets):\n",
    "        \"\"\"\n",
    "        Test the model.\n",
    "        Outputs the overall accuracy of the model.\n",
    "        \"\"\"\n",
    "        cummulative_precisions = 0\n",
    "        for i in range(len(input_vectors)):\n",
    "            prediction = self.predict(input_vectors[i])\n",
    "            precision = 1 - ((prediction - targets[i]) / targets[i])\n",
    "            cummulative_precisions = cummulative_precisions + precision\n",
    "        return cummulative_precisions / len(input_vectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insantitate the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "model = MyModel(alpha)\n",
    "model.predict(dataset_test[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is pretty horrible right now, but let's train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_errors = model.train(dataset_train, targets_train, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cummulative_errors)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error for all training instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_precision = model.fit(dataset_test, targets_test)\n",
    "print(f\"Model precision through test dataset is {model_precision}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, through training and implementing different activation functions, we have reached an accuracy of *about* **71-74%**. Still not economically optimal, but it is a good start."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- Preprocessing input data is important\n",
    "\n",
    "- Input layer of the neural network is crucial. For large values, it is wise to use functions that reduces them to a result that can be efficiently passed to the activation function, while still witholding the relationships between all variables.\n",
    "\n",
    "- For example, at the beginning I had planned to use **dot product** of the input vector and weight vector, but the result turned out to be too large (or too small), which causes the activation function (Sigmoid's) to be inconclusive. By replacing layer 1 with the **cosine_similarity** instead - with a range of (-1, 1) - and extending the range to (-5, 5), I was able to utilize the Sigmoid's activation function.\n",
    "\n",
    "- Clearly, this model can still be improved drastically. Mind the fact that the training dataset is still quite small (only 5000 entries)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
